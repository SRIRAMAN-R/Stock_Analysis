{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772344d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\Sriraman\\AppData\\Local\\Temp\\ipykernel_24708\\4017541062.py:7: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  DATA_DIR = 'D:\\Mainboot Project\\stock\\data'\n",
      "Processing Dates: 100%|██████████| 305/305 [00:19<00:00, 15.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All symbol-wise CSV files saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_DIR = 'D:\\Mainboot Project\\stock\\data'\n",
    "OUTPUT_DIR = 'output1_csv'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "date_range = pd.date_range(start='2023-10-01', end='2024-11-30', freq='B')  \n",
    "\n",
    "symbol_data = {}\n",
    "\n",
    "for dt in tqdm(date_range, desc=\"Processing Dates\"):\n",
    "    try:\n",
    "        month_folder = dt.strftime(\"%Y-%m\")\n",
    "        date_str = dt.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        file_name = f\"{dt.strftime('%Y-%m-%d')}_05-30-00.yaml\"\n",
    "        file_path = os.path.join(DATA_DIR, month_folder, file_name)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = yaml.safe_load(file)\n",
    "            if not data:\n",
    "                continue\n",
    "\n",
    "            for record in data:\n",
    "                symbol = record.get('Ticker')\n",
    "                if not symbol:\n",
    "                    continue\n",
    "\n",
    "                if symbol not in symbol_data:\n",
    "                    symbol_data[symbol] = []\n",
    "\n",
    "                symbol_data[symbol].append({\n",
    "                    'date': record.get('date'),\n",
    "                    'open': record.get('open'),\n",
    "                    'high': record.get('high'),\n",
    "                    'low': record.get('low'),\n",
    "                    'close': record.get('close'),\n",
    "                    'volume': record.get('volume'),\n",
    "                    'month': record.get('month')\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Save CSV files\n",
    "for symbol, records in symbol_data.items():\n",
    "    df = pd.DataFrame(records)\n",
    "    df.sort_values(by='date', inplace=True)\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"{symbol}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"All symbol-wise CSV files saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed820cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning CSVs: 100%|██████████| 50/50 [00:01<00:00, 25.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All cleaned CSV files saved with 'month' column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_DIR = 'output1_csv'\n",
    "CLEANED_DIR = 'cleaned_csv'\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "\n",
    "csv_files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.csv')]\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Cleaning CSVs\"):\n",
    "    try:\n",
    "        file_path = os.path.join(INPUT_DIR, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df['open'] = pd.to_numeric(df['open'], errors='coerce')\n",
    "        df['high'] = pd.to_numeric(df['high'], errors='coerce')\n",
    "        df['low'] = pd.to_numeric(df['low'], errors='coerce')\n",
    "        df['close'] = pd.to_numeric(df['close'], errors='coerce')\n",
    "        df['volume'] = pd.to_numeric(df['volume'], errors='coerce')\n",
    "\n",
    "        if 'month' not in df.columns or df['month'].isnull().all():\n",
    "            df['month'] = df['date'].dt.to_period('M').astype(str)  # Fallback if month is missing\n",
    "        else:\n",
    "            df['month'] = df['month'].astype(str)\n",
    "\n",
    "        df.dropna(inplace=True)\n",
    "        df.sort_values(by='date', inplace=True)\n",
    "        df.drop_duplicates(subset=['date'], keep='last', inplace=True)\n",
    "        cleaned_path = os.path.join(CLEANED_DIR, file)\n",
    "        df.to_csv(cleaned_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning {file}: {e}\")\n",
    "\n",
    "print(\"All cleaned CSV files saved with 'month' column.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69573a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining CSVs: 100%|██████████| 50/50 [00:01<00:00, 33.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined CSV saved as: combined_stocks.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "CLEANED_DIR = 'cleaned_csv'\n",
    "COMBINED_CSV = 'combined_stocks.csv'\n",
    "\n",
    "all_data = []\n",
    "\n",
    "csv_files = [f for f in os.listdir(CLEANED_DIR) if f.endswith('.csv')]\n",
    "\n",
    "for file in tqdm(csv_files, desc=\"Combining CSVs\"):\n",
    "    file_path = os.path.join(CLEANED_DIR, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    symbol = os.path.splitext(file)[0]\n",
    "    df['symbol'] = symbol\n",
    "\n",
    "    all_data.append(df)\n",
    "\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "combined_df.sort_values(by=['date', 'symbol'], inplace=True)\n",
    "combined_df.to_csv(COMBINED_CSV, index=False)\n",
    "\n",
    "print(f\"Combined CSV saved as: {COMBINED_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Yearly Returns, Top Gainers & Losers, Market Summary\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('combined_stocks.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df[['open', 'high', 'low', 'close', 'volume']] = df[['open', 'high', 'low', 'close', 'volume']].apply(pd.to_numeric, errors='coerce')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "first_close = df.sort_values(by='date').groupby('symbol').first()['close']\n",
    "last_close = df.sort_values(by='date').groupby('symbol').last()['close']\n",
    "\n",
    "yearly_return_df = pd.DataFrame({\n",
    "    'symbol': first_close.index,\n",
    "    'first_close': first_close.values,\n",
    "    'last_close': last_close.values,\n",
    "    'yearly_return': ((last_close.values - first_close.values) / first_close.values) * 100\n",
    "})\n",
    "yearly_return_df.to_csv('yearly_returns.csv', index=False)\n",
    "\n",
    "# Top 10 Green & Loss Stocks\n",
    "yearly_return_df.sort_values(by='yearly_return', ascending=False).head(10).to_csv('top_10_green_stocks.csv', index=False)\n",
    "yearly_return_df.sort_values(by='yearly_return', ascending=True).head(10).to_csv('top_10_loss_stocks.csv', index=False)\n",
    "\n",
    "# Market Summary\n",
    "market_summary_df = pd.DataFrame([{\n",
    "    'green_stocks': (yearly_return_df['yearly_return'] > 0).sum(),\n",
    "    'red_stocks': (yearly_return_df['yearly_return'] <= 0).sum(),\n",
    "    'avg_close_price': round(df['close'].mean(), 2),\n",
    "    'avg_volume': int(df['volume'].mean())\n",
    "}])\n",
    "market_summary_df.to_csv('market_summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Volatility (Top 10 Volatile Stocks)\n",
    "\n",
    "df['daily_return'] = df.groupby('symbol')['close'].pct_change()\n",
    "volatility = df.groupby('symbol')['daily_return'].std().reset_index()\n",
    "volatility.columns = ['symbol', 'volatility']\n",
    "volatility.dropna(inplace=True)\n",
    "volatility.sort_values(by='volatility', ascending=False).head(10).to_csv('top_10_volatile_stocks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847507ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sriraman\\AppData\\Local\\Temp\\ipykernel_16884\\3730455587.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_5_df['date'] = top_5_df['date'].dt.date\n"
     ]
    }
   ],
   "source": [
    "# 3. Cumulative Return by Date (Top 5 Stocks)\n",
    "\n",
    "df['daily_return'] = df.groupby('symbol')['close'].pct_change().fillna(0)\n",
    "df['cumulative_return'] = df.groupby('symbol')['daily_return'].transform(lambda x: (1 + x).cumprod() - 1)\n",
    "\n",
    "final_returns = df.groupby('symbol')['cumulative_return'].last().reset_index(name='final_cumulative_return')\n",
    "top_5_symbols = final_returns.sort_values(by='final_cumulative_return', ascending=False).head(5)['symbol']\n",
    "\n",
    "top_5_df = df[df['symbol'].isin(top_5_symbols)]\n",
    "top_5_df['date'] = top_5_df['date'].dt.date\n",
    "top_5_df[['date', 'symbol', 'cumulative_return']].to_csv('cumulative_returns_by_date.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Sector-wise Performance\n",
    "returns = df.groupby('symbol').agg(\n",
    "    first_close=('close', 'first'),\n",
    "    last_close=('close', 'last')\n",
    ").reset_index()\n",
    "returns['cumulative_return'] = (returns['last_close'] - returns['first_close']) / returns['first_close']\n",
    "\n",
    "sector_df = pd.read_csv(\"Sector_data - Sheet1.csv\")\n",
    "sector_df['symbol'] = sector_df['Symbol'].str.split(':').str[-1].str.strip()\n",
    "\n",
    "returns = pd.merge(returns, sector_df[['sector', 'symbol']], on='symbol', how='left')\n",
    "returns.dropna(subset=['sector'], inplace=True)\n",
    "\n",
    "returns.groupby('sector')['cumulative_return'].mean().reset_index(name='average_return')\\\n",
    "    .sort_values(by='average_return', ascending=False).to_csv(\"sector_wise_performance.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSVs created successfully.\n"
     ]
    }
   ],
   "source": [
    "# 5. Stock Correlation\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.sort_values(by=['symbol', 'date'], inplace=True)\n",
    "\n",
    "df['daily_return'] = df.groupby('symbol')['close'].pct_change()\n",
    "pivot_df = df.pivot(index='date', columns='symbol', values='daily_return')\n",
    "\n",
    "correlation_matrix = pivot_df.corr()\n",
    "correlation_matrix.to_csv('correlation_matrix.csv')  \n",
    "\n",
    "flat_corr = correlation_matrix.unstack().rename_axis(['Stock_1', 'Stock_2']).reset_index(name='correlation')\n",
    "flat_corr = flat_corr[flat_corr['Stock_1'] != flat_corr['Stock_2']]\n",
    "flat_corr['pair'] = flat_corr.apply(lambda x: tuple(sorted([x['Stock_1'], x['Stock_2']])), axis=1)\n",
    "flat_corr = flat_corr.drop_duplicates('pair').drop(columns='pair')\n",
    "\n",
    "flat_corr.to_csv('stock_price_correlation_pairs.csv', index=False)\n",
    "\n",
    "print(\"All CSVs created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Monthly Top 5 Gainers & Losers\n",
    "df['month'] = df['date'].dt.to_period('M')\n",
    "monthly_returns = df.groupby(['symbol', 'month'])['close'].agg(['first', 'last']).reset_index()\n",
    "monthly_returns['monthly_return'] = (monthly_returns['last'] - monthly_returns['first']) / monthly_returns['first']\n",
    "\n",
    "top5_gainers, top5_losers = [], []\n",
    "for month in monthly_returns['month'].unique():\n",
    "    data = monthly_returns[monthly_returns['month'] == month]\n",
    "    top5_gainers.append(data.nlargest(5, 'monthly_return').assign(rank=range(1, 6), month=month))\n",
    "    top5_losers.append(data.nsmallest(5, 'monthly_return').assign(rank=range(1, 6), month=month))\n",
    "\n",
    "gainers_df = pd.concat(top5_gainers)[['month', 'symbol', 'monthly_return', 'rank']]\n",
    "losers_df = pd.concat(top5_losers)[['month', 'symbol', 'monthly_return', 'rank']]\n",
    "\n",
    "gainers_df.to_csv(\"monthly_top5_gainers.csv\", index=False)\n",
    "losers_df.to_csv(\"monthly_top5_losers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf101fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database 'stocks_analysis' created or already exists.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "initial_conn = mysql.connector.connect(\n",
    "    host='gateway01.ap-southeast-1.prod.aws.tidbcloud.com',\n",
    "    user='2Djg5GyoVwWC4gN.root',\n",
    "    password='JWYIDlQjiVFU2pUD',\n",
    "    port=4000\n",
    ")\n",
    "initial_cursor = initial_conn.cursor()\n",
    "\n",
    "initial_cursor.execute(\"CREATE DATABASE IF NOT EXISTS stocks_analysis\")\n",
    "print(\"atabase 'stocks_analysis' created or already exists.\")\n",
    "initial_cursor.close()\n",
    "initial_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48782a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded `top_10_volatile_stocks.csv` into `top_10_volatile_stocks`\n",
      "✅ Loaded `yearly_returns.csv` into `yearly_returns`\n",
      "✅ Loaded `top_10_loss_stocks.csv` into `top_10_loss_stocks`\n",
      "✅ Loaded `top_10_green_stocks.csv` into `top_10_green_stocks`\n",
      "✅ Loaded `stock_price_correlation_pairs.csv` into `stock_price_correlation_pairs`\n",
      "✅ Loaded `sector_wise_performance.csv` into `sector_wise_performance`\n",
      "✅ Loaded `monthly_top5_gainers.csv` into `monthly_top5_gainers`\n",
      "✅ Loaded `monthly_top5_losers.csv` into `monthly_top5_losers`\n",
      "✅ Loaded `market_summary.csv` into `market_summary`\n",
      "✅ Loaded `cumulative_returns_by_date.csv` into `cumulative_returns_by_date`\n",
      "✅ Loaded `correlation_matrix.csv` into `correlation_matrix`\n",
      "🎉 All CSVs loaded into TiDB successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "    host='gateway01.ap-southeast-1.prod.aws.tidbcloud.com',       \n",
    "    user='2Djg5GyoVwWC4gN.root',\n",
    "    password='JWYIDlQjiVFU2pUD',\n",
    "    database='stocks_analysis',\n",
    "    port=4000                    \n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "csv_files = [\n",
    "    \"top_10_volatile_stocks.csv\",\n",
    "    \"yearly_returns.csv\",\n",
    "    \"top_10_loss_stocks.csv\",\n",
    "    \"top_10_green_stocks.csv\",\n",
    "    \"stock_price_correlation_pairs.csv\",\n",
    "    \"sector_wise_performance.csv\",\n",
    "    \"monthly_top5_gainers.csv\",\n",
    "    \"monthly_top5_losers.csv\",\n",
    "    \"market_summary.csv\",\n",
    "    \"cumulative_returns_by_date.csv\",\n",
    "    \"correlation_matrix.csv\"\n",
    "    \n",
    "]\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    if not os.path.exists(csv_file):\n",
    "        print(f\"File not found: {csv_file}\")\n",
    "        continue\n",
    "\n",
    "    table_name = os.path.splitext(os.path.basename(csv_file))[0].lower()\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS `{table_name}`\")\n",
    "\n",
    "    columns = []\n",
    "    for col, dtype in zip(df.columns, df.dtypes):\n",
    "        if 'int' in str(dtype):\n",
    "            sql_type = 'INT'\n",
    "        elif 'float' in str(dtype):\n",
    "            sql_type = 'FLOAT'\n",
    "        elif 'datetime' in str(dtype):\n",
    "            sql_type = 'DATETIME'\n",
    "        else:\n",
    "            sql_type = 'VARCHAR(255)'\n",
    "        columns.append(f\"`{col}` {sql_type}\")\n",
    "    create_query = f\"CREATE TABLE `{table_name}` ({', '.join(columns)})\"\n",
    "    cursor.execute(create_query)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        values = tuple(None if pd.isna(x) else str(x) for x in row)\n",
    "        placeholders = ', '.join(['%s'] * len(values))\n",
    "        insert_query = f\"INSERT INTO `{table_name}` VALUES ({placeholders})\"\n",
    "        cursor.execute(insert_query, values)\n",
    "\n",
    "    conn.commit()\n",
    "    print(f\"Loaded `{csv_file}` into `{table_name}`\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"All CSVs loaded into TiDB successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
